{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Neural Network     \n",
    "\n",
    "## Binary Classification\n",
    "## Logistic Regression\n",
    "### Sigmoid function\n",
    "$$ \\hat{y}=P(y=1 | x)=\\sigma(w^Tx + b),  0\\leq \\hat{y}\\leq 1$$\n",
    "\n",
    "### Cost function\n",
    "For one sample:  \n",
    "Square Error:  \n",
    "$$Loss(\\hat{y},y)=\\frac{1}{2}(\\hat{y}- y)^2$$\n",
    "It makes GD not work well.  \n",
    "**Information Cross Entropy**:(A Better choice)  \n",
    "$$Loss(\\hat{y},y)=-(y\\log{\\hat{y}}+(1-y)\\log{(1-\\hat{y})})$$\n",
    "For the dataset, we calculate the mean of the error:  \n",
    "$$J(w,b)=\\frac{1}{m}\\sum_{i}^m Loss(\\hat{y}^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We want to **minimize J(w,b) **   \n",
    "Algorithm : seen in Zhouzhihua ML book  \n",
    "Derivative in a Computation Graph:  chain rule  \n",
    "$$ z = w^Tx + b,$$\n",
    "$$\\hat y=\\sigma {(z)}=\\sigma{ (w^Tx + b)}$$\n",
    "You can deduce that:(if using cross entropy)    \n",
    "$$\\frac{\\partial L}{\\partial z} = \\hat y-y$$\n",
    "Hence  \n",
    "$$\\frac{\\partial L}{\\partial w_i}=x_i\\frac{\\partial L}{\\partial z}\\quad $$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b}=\\frac{\\partial L}{\\partial z}\\quad $$\n",
    "$$w_i=w_i-\\alpha \\frac{\\partial L}{\\partial w_i}$$\n",
    "\n",
    "## For m samples:\n",
    "### Vectorization: get rid of 'for' loop in code    \n",
    "In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249951.474113\n",
      "Vectorization version:2.001047134399414 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "a=np.random.random(1000000)\n",
    "b=np.random.random(1000000)\n",
    "\n",
    "#np.dot(w,x)  #realize $w^Tx + b$\n",
    "\n",
    "tic =time.time()\n",
    "z = np.dot(a,b)\n",
    "toc=time.time()\n",
    "print(z)\n",
    "print('Vectorization version:'+ str(1000*(toc-tic))+' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare with the loop method , the vectorization is much faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we go on to focus on m-sample question  \n",
    "we assume $\\frac{\\partial L}{\\partial z} = \\hat y-y = \\mathrm{d}z$  in one sample  \n",
    "so we build the matrix that each column vector is one-sample derivative of z\n",
    "$$dz = [dz^{(1)},dz^{(2)},...dz^{(m)}]$$\n",
    "X is the imput matrix composed with column vectors $x^{(1)},x^{(2)},...$  \n",
    " $$dw=\\frac{\\partial J}{\\partial w}=1/m * X (dz)^T$$\n",
    " $$db=\\frac{\\partial J}{\\partial b}=1/m * np.sum(dz)$$\n",
    "the result is a \n",
    "And you still need a loop to do GD on the same dataset and the loop is inevitable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19161645,  0.1538191 ,  0.70168727,  0.19568387]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.randn(1,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 100.19161645,  100.1538191 ,  100.70168727,  100.19568387]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.19161645,  0.1538191 ],\n",
       "       [ 0.70168727,  0.19568387]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b= a.reshape((2,2))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89330372,  0.34950298])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 100.19161645,  100.1538191 ],\n",
       "       [ 200.70168727,  200.19568387]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b+np.array([[100],[200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For layer k  ,the weights we assign $\\mathbf{W^{[k]}}$  \n",
    "the activation input is $z^{[k]}$   ,partial derivative of $z^{[k]}$ is $dz^{[k]}$  \n",
    "the activation output is $A^{[k]}=[a^{[k]}_0,a^{[k]}_1,a^{[k]}_2...]^T$ , and we assume that $ A^{[0]}= X $ \n",
    "layer k has $n^{[k]}$ nodes  \n",
    "Note : the number of layers of the network : normally donot count the input layer(layer 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation of the neural network is clear, and let's focus on the vectorization implementation  \n",
    "m samples computation using matrix computing  \n",
    "For example\n",
    "$$W^{[1]}X+b^{[1]}=z^{[1]} $$ for the following layer is \n",
    "$$W^{[k]}A^{[k-1]}+b^{[k]}=z^{[k]}$$\n",
    "X is composed with column vector $x^{(1)},x^{(2)},...$\n",
    "$$A^{[k]}=[a^{[k](1)},a^{[k](2)},a^{[k](3)},...,a^{[k](m)}],shape=(n^{[k]},m)$$\n",
    "Hence  \n",
    "<font color=red size=5>\n",
    "$$dW^{[k]}=\\frac{\\partial J}{\\partial W^{[k]}}=\\frac{1}{m}  dz^{[k]}(A^{[k-1]})^T $$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "non-linear function  \n",
    "#### Most common activation func :   \n",
    "- sigmoid (between 0 and 1)\n",
    "- tanh (between -1 and 1) the averge output is 0 (benefit,superer than sigmoid)\n",
    "- Relu (Rectified linear unit) $ a = max(0,z)$\n",
    "- leaky Relu (based on Relu ,when input is negative, the derivative is no longer 0, but a very small positive value) $a=max(0.01z,z)$  \n",
    "\n",
    "if don't know which is better , can evaluate them on a holdout validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivative of activation func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "func: g  \n",
    "- sigmoid: $a=g(z)$,$g'(z)=a(1-a)$  \n",
    "- tanh: $a=g(z)$,$g'(z)=1-a^2$  \n",
    "- Relu: $g'(z)=0(if\\quad z<0)\\quad=1(if \\quad z>0)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### BP algorithm in NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two layer NN,  \n",
    "We assume that $n^{[0]}$ is the number of input x feat ,$n^{[1]}$ is the number of units in hidden layer,$n^{[2]}=1$ is the number of outputs  \n",
    "So the shape of $W^{[1]}$ is $$(n^{[1]},n^{[0]})$$\n",
    "the shape of $b^{[1]}$ is $$(n^{[1]},1)$$\n",
    "the shape of X is $$(n^{[0]},m)$$\n",
    "the shape of $W^{[2]}$ is $$(n^{[2]},n^{[1]})$$\n",
    "the shape of $b^{[2]}$ is $$(n^{[2]},1)$$\n",
    "Hence the shape of output $A^{[2]}$ is $$(n^{[2]},m)$$\n",
    "Forward Propagation: shown above  \n",
    "Back Propagation: \n",
    "$$dz^{[2]}=A^{[2]}-y, A^{[2]}=\\hat y $$\n",
    "$$y=[y^{(1)},y^{(2)},y^{(3)},...y^{(m)}]\\quad shape:(n^{[2]},m)$$\n",
    "\n",
    "$$db^{[k]}=\\frac{1}{m}np.sum(dz^{[k]},axis=1,keepdims=True)$$\n",
    "we use (keepdims = True) to make sure that A.shape is (dim,1) and not (dim, ). It makes our code more rigorous\n",
    "<font color=red size=5>\n",
    "$$ dz^{[1]}=\\frac{\\partial J}{\\partial z^{[2]}}\\frac{\\partial z^{[2]}}{\\partial A^{[1]}}\\frac{\\partial A^{[1]}}{\\partial z^{[1]}}\\\\ =(W^{[2]})^Tdz^{[2]} * g'(z^{[1]})$$\n",
    "</font>\n",
    "<center> it can be seen as $dz^{[k]}=dA^{[k]}\\cdot g'(z^{[k]})$  \n",
    "$dA^{[k-1]}$ can be deduced from $dz^{[k]}$ and $dz^{[k]}$ can be deduced from $dA^{[k]}$, and this realizes the back prop   \n",
    "the * is element-wise product ,cus the shape of operandant are both $(n^{[1]},m)$</center>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can calculate the dz,db to derive dW using the formulas derived above using red color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the weight randomly\n",
    "\n",
    "Disadvantage of initialize the weight to zero  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W1=np.random.randn(n[1],n[0]) * 0.01  \n",
    "initialize it to a very small random value  \n",
    "Why??  \n",
    "If the weight is large , the activation value is very large and it falls on the flat part of the activation func ,leading to the small gradient, and the gradient descent will be very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Forward propagation vectorization:\n",
    "it's ok to implement a loop to compute z and A  \n",
    "### Check the matrix dimension\n",
    "For layer k:  \n",
    "Shape of $W^{[k]}$ is $$(n^{[k]},n^{[k-1]})$$\n",
    "Shape of $b^{[k]}$ is $$(n^{[k]},1)$$\n",
    "the dimension of dw should be the same as the dimension of W, as well as db to b  \n",
    "z and A should have the same dimension  $$(n^{[k]},m)$$\n",
    "### Why we build a 'deep' network\n",
    "have the earlier layers learn these low-level simpler features and in the deep layers we put together the simple features that detected  \n",
    "In curcuit theory, some logistic gates'function can compute with a relatively small but deep network.\n",
    "### Building blocks of deep NN\n",
    "It will be very useful to store the $z^{[k]}$  for both forward and back prop.(include this on cache )\n",
    "grads and $A^{[k]}$  \n",
    "<img src=\"images/forward_and_back_prop.png\" style=\"width:600px;height:300px;\">\n",
    "the blocks in back propagation will output the grads--dw and db  \n",
    "### Implement the blocks of back propagation vectorization:\n",
    "Forward prop for layer k:  \n",
    "Input : $A^{[k-1]}$  \n",
    "Output: $A^{[k]}$  cache: $z^{[k]}$ (it can be calculated from $W^{[k]},b^{[k]}$  \n",
    "from k=1, A[0]=X is fed. And implement the forward prop ,cache z . \n",
    "\n",
    "Back prop for layer k:  \n",
    "Input : $dA^{[k]}$  \n",
    "Output: $dA^{[k-1]},dW^{[k]},db^{[k]}$  \n",
    "<font color=blue size=5>\n",
    "$$dz^{[k]}=dA^{[k]}\\cdot g'(z^{[k]})$$\n",
    "\n",
    "$$dW^{[k]}=\\frac{\\partial J}{\\partial W^{[k]}}=\\frac{1}{m}  dz^{[k]}(A^{[k-1]})^T $$\n",
    "\n",
    "$$db^{[k]}=\\frac{1}{m}np.sum(dz^{[k]},axis=1,keepdims=True)$$\n",
    "\n",
    "$$dA^{[k-1]}=(W^{[k]})^Tdz^{[k]}$$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Outline of the deep NN\n",
    "<img src=\"images/final outline.png\" style=\"width:600px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize hyperparameters well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters :   \n",
    "1. Learning rate  \n",
    "2. Number of iterations   \n",
    "3. Number of hidden layers  \n",
    "4. Hidden units of each hidden layer  \n",
    "5. Choice of activation function  \n",
    "These control W and b ,so call them hyperparameter "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
